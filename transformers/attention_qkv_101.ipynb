{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers and the Attention Mechanism\n",
    "\n",
    "I'm learning about transformers as I've only ever really learnt about them in theory, and instead did the mathematical stuff on earlier architectures like RNNs, LSTMs, CNNs, linear models, etc., which have just been totally devoured by the general purposeness and stupid effectieness of transformers.\n",
    "\n",
    "Given how important it is, I thought I'd get my hands dirty, and follow what Karpathy did (except in python) and code one up from scratch to try to learn this tech better.\n",
    "\n",
    "First thing to understand is how is it different from other architectures? \n",
    "\n",
    "Well, one of the key differences is the **attention mechanism**. So what the attention mechanism? It's got a bit of history, but I'll just go into what they are in transformers.\n",
    "\n",
    "## The Attention Mechanism\n",
    "\n",
    "Attention has to do with one special feature of transformers, and that's the contextualised representation of inputs (e.g. words).\n",
    "\n",
    "In word2vec, for example, words are represented by **static** vectors, which dont change depending on their context around them. That means, for example, a river 'bank' and a memory 'bank' are represented with the same vector. \n",
    "\n",
    "In transformers however, the context matters! The vector represenation of a word changes depending on the words around it. How does it do this? Through attention!\n",
    "\n",
    "Attention tells us what other words to focus on for an input word (we'll keep using words as inputs here for a while, but note that's not necessary or fundamental). It does this using three other vectors known as the **query, key, and value** vectors, and some matrix maths.\n",
    "\n",
    "## QKV Matrices\n",
    "\n",
    "The query vector represents the word we're asking about in question: It's the word we're hoping to find relevant other words for, or that we are 'attending to'. The key vector is the representation of the word we search against, and the value vector is what the word 'represents', btu is not simply the input embedding, it is that in context. \n",
    "\n",
    "An attention score is the similarity between a query vector and a key vector, which is calculated between one wod and all other words. \n",
    "\n",
    "These vectors are created as linear transformations of input embeddings, but we'll get to how those are calculated later.\n",
    "\n",
    "So for example, in the sentence\n",
    "\n",
    "\"The cat sat\", each word has three vectors, which may look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries:\n",
      " [[2 1 2]\n",
      " [0 1 0]\n",
      " [2 2 2]]\n",
      "Keys:\n",
      " [[0 1 2]\n",
      " [1 0 0]\n",
      " [1 1 2]]\n",
      "Values:\n",
      " [[2 3 1]\n",
      " [0 1 0]\n",
      " [2 4 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Input embeddings for \"I love coding\"\n",
    "embeddings = np.array([\n",
    "    [1, 0, 1], # I\n",
    "    [0, 1, 0], # love\n",
    "    [1, 1, 1], # coding\n",
    "])\n",
    "\n",
    "# Predefined weights for query, key, and value vectors\n",
    "W_query = np.array([\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [1, 1, 1]\n",
    "])\n",
    "W_key = np.array([\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "W_value = np.array([\n",
    "    [1, 2, 1],\n",
    "    [0, 1, 0],\n",
    "    [1, 1, 0]\n",
    "])\n",
    "\n",
    "# Step 1: Transform input embeddings into the Query, Key, and Value vectors\n",
    "queries = embeddings @ W_query\n",
    "keys = embeddings @ W_key\n",
    "values = embeddings @ W_value\n",
    "\n",
    "# Print these values \n",
    "print(\"Queries:\\n\", queries)\n",
    "print(\"Keys:\\n\", keys)\n",
    "print(\"Values:\\n\", values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we too some input embedding vectors, and some weight matrices, and multiplied them out to get some query, key, and value vectors, one for each input word. \n",
    "\n",
    "Next, we want to do the attention calculation, which once again calculates the relevance of a word to other words in a context, by calculating the dot product (a similarity measurement between vectors) between the query vector and the other key vectors. \n",
    "\n",
    "One question you might have is, why aren't the query vectors and the key vectors for a word the same if we're just using them to look up similarities between words in context?\n",
    "\n",
    "This is because query vectors represents what the word is **'searching'** for, and the key vector is what a word **'has to offer the searcher'**. This separation is key to allowing the same word to behave differently in different contexts.\n",
    "\n",
    "The query and key weight matrices in transformers are learned during trainingt to optimise the attention mechanism, and it turns out that by doing this, by specialisign the roles of the vectors in a different way than just measuring vector similarity but instead an affinity between a query and a key vector, we get better results. It's \n",
    "\n",
    "Now we have that down, let's calculate the attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Attention Scores for 'love': [1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Select the query for the word \"love\"\n",
    "query_love = queries[1]  # Index 1 corresponds to \"love\"\n",
    "\n",
    "# Step 3: Compute dot products of the query with all keys\n",
    "attention_scores = keys @ query_love  # Matrix-vector multiplication\n",
    "\n",
    "# Print the raw attention scores\n",
    "print(\"Raw Attention Scores for 'love':\", attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so this is telling us that the word love has a raw attention score of 1 with I, 0 with \"love\", and 1 with \"coding\". \n",
    "\n",
    "But this on its own isn't all that useful if we want to use it to use it as a kind of scaling factor. \n",
    "\n",
    "To turn this vector into one where each element represents a weight and all weights sum to 1, we can use **softmax**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Probabilities for 'love': [0.4223188 0.1553624 0.4223188]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Apply softmax function to the raw attention scores\n",
    "def softmax(scores):\n",
    "    exp_scores = np.exp(scores - np.max(scores))    # Shift vector to a range between [0, max(scores)]\n",
    "    return exp_scores / np.sum(exp_scores)  # Ensure all explonentiated scores add up to 1\n",
    "\n",
    "attention_probs = softmax(attention_scores)\n",
    "\n",
    "# Print normalised attention probabilities\n",
    "print(\"Attention Probabilities for 'love':\", attention_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's going on here? We see that higher raw attention scores are boosted relative to low attention scores, which are relatively suppressed.\n",
    "\n",
    "Okay, so next we will use these probabilities to **weight the value vectors** and aggregate them into a single output vector for \"love\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output vector for 'love': [1.68927519 3.11159399 0.8446376 ]\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Calculate output vector for \"love\"\n",
    "output_love = attention_probs @ values\n",
    "print(\"Output vector for 'love':\", output_love)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output vector represents the input vector 'love', but **contextualised by the words \"I\" and \"coding\"**\n",
    "\n",
    "## Summary\n",
    "We covered a bit about the attention mechanism in transformers, why it's useful, how we use the query key and value matrices to calculate attention and the resulting contextualised output vector. Great job! \n",
    "\n",
    "Next up will probably be how the QKV weight matrices get calculated in training. \n",
    "\n",
    "Thanks for reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
